{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fada139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from DataExtractor.downloader import download_dataset\n",
    "from DataExtractor.dataset_to_dataframe import ConvertICDARDatasetToDataframe\n",
    "from DataAugmentation.augmentation import Augmentation\n",
    "from DataExtractor.yolo_converter import ICDARYOLOConverter\n",
    "from DataSplitter.kfold import DataFrameKFoldSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4acf45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d89857ae",
   "metadata": {},
   "source": [
    "# Download do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25704979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando o dataset... espere!\n",
      "Baixado 6125166592 bytes de indefinido...\n",
      "Extraindo dataset...\n",
      "Download do dataset!\n"
     ]
    }
   ],
   "source": [
    "# realiza o download do dataset\n",
    "\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad2b20",
   "metadata": {},
   "source": [
    "# Extração de Metadados em DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be723b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Associando pares... : 100%|██████████| 600/600 [00:00<00:00, 727.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando DataFrame do conjnuto de dados...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo dimensão das imagens...: 100%|██████████| 600/600 [00:11<00:00, 53.22it/s]\n",
      "Extraindo as anotações...: 100%|██████████| 600/600 [00:13<00:00, 43.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# gera um dataframe contendo metadados sobre o dataset como o caminhos até as imagens, ss dimensões das imagens, \n",
    "# as anotações dos pontos (x,y) dos vértices dos polígonos das máscaradas extraídas a partir dos arquivos XML, etc.\n",
    "\n",
    "converter = ConvertICDARDatasetToDataframe(\n",
    "    images_path=\"./dataset/training/TRACKB1/ground_truth\",\n",
    "    labels_path=\"./dataset/training/TRACKB1/ground_truth\"\n",
    ")\n",
    "\n",
    "df = converter.generate_dataframe()\n",
    "df.to_csv('dataset.csv',  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2335e6",
   "metadata": {},
   "source": [
    "# Redimensionamento das Imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9570a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realiza a leitura dos metadados do conjunto de dados no arquivo dataset.csv\n",
    "dataset_df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# cria uma nova pasta para armazenar as imagens redimensionadas \n",
    "os.makedirs('resized_dataset', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb481bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redimensionando as imagens e anotaçãoes para 640X640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convertendo imagem cTDaR_t00930.jpg (2480X3830) -> (640X640): 100%|██████████| 600/600 [01:21<00:00,  7.40it/s] \n"
     ]
    }
   ],
   "source": [
    "# Redimensiona as imagens para o tamanho 640x640, além de redimensionar as anotações respectivamente\n",
    "\n",
    "new_width, new_height = 640, 640\n",
    "total_rows = dataset_df.shape[0]\n",
    "\n",
    "new_masks = []\n",
    "new_paths = []\n",
    "\n",
    "print(f'Redimensionando as imagens e anotaçãoes para {new_width}X{new_height}')\n",
    "\n",
    "pbar = tqdm(dataset_df.iterrows(),total=dataset_df.shape[0])\n",
    "\n",
    "for _, dataset_row in pbar: \n",
    "\n",
    "    \n",
    "    image_path  = dataset_row['image_path']\n",
    "    masks = json.loads(dataset_row['xy'])\n",
    "\n",
    "    image_filename = os.path.basename(image_path)\n",
    "    output_path = f'./resized_dataset/{image_filename}'\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = np.asarray(image)\n",
    "\n",
    "    pbar.set_description(f'Convertendo imagem {image_filename} ({image.shape[1]}X{image.shape[0]}) -> ({new_width}X{new_height})')\n",
    "    \n",
    "    resized_image, resized_masks = \\\n",
    "        Augmentation.resize_image(image, new_width, new_height, masks=masks)\n",
    "    \n",
    "    new_masks.append(resized_masks)\n",
    "    new_paths.append(output_path)\n",
    "\n",
    "    Augmentation.save_image(resized_image, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a30a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove o diretório do dataset original, já que ele não será necessário\n",
    "shutil.rmtree('dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b3caaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarta o arquivo dataset.csv, já que ele não é necessário também\n",
    "os.remove('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a72b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# armazena os metadados do conjunto redimensionado em um arquivo csv\n",
    "resized_dataset_df = pd.DataFrame(\n",
    "    {'image_path':new_paths,\n",
    "    'xy':new_masks}\n",
    ")\n",
    "\n",
    "resized_dataset_df['image_width'] = new_width\n",
    "resized_dataset_df['image_height'] = new_height\n",
    "\n",
    "resized_dataset_df.to_csv('resized_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57c805",
   "metadata": {},
   "source": [
    "# Geração das Anotações YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb9254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leitura do metadados do conjunto de dados redimensionado\n",
    "resized_dataset_df = pd.read_csv('resized_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3bd9ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando anotações YOLO em txt...: 100%|██████████| 600/600 [00:01<00:00, 316.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# geração dos rótulos YOLO em arquivos txt no mesmo diretório das respectivas imagens\n",
    "\n",
    "new_masks = []\n",
    "txt_paths = []\n",
    "\n",
    "for _, resized_dataset_row in tqdm(resized_dataset_df.iterrows(),  \n",
    "                                   total = resized_dataset_df.shape[0],\n",
    "                                   desc='Gerando anotações YOLO em txt...') :\n",
    "    \n",
    "    image_width = resized_dataset_row['image_width']\n",
    "    image_height = resized_dataset_row['image_height']\n",
    "    image_path = resized_dataset_row['image_path']\n",
    "    masks = resized_dataset_row['xy']\n",
    "    masks = json.loads(masks)\n",
    "\n",
    "    image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    image_path = Path(os.path.dirname(image_path))\n",
    "    txt_path = image_path/f'{image_filename}.txt' \n",
    "\n",
    "    \n",
    "    yolo_normalized_masks = ICDARYOLOConverter.process_masks(masks, image_width, image_height, txt_path)\n",
    "\n",
    "    new_masks.append(yolo_normalized_masks)\n",
    "    txt_paths.append(txt_path.as_posix())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f38ae201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adiciona as máscarar convertidas para o formato YOLO e os respectivos caminhos para os arquivos TXT com as anotações salva\n",
    "resized_dataset_df['yolo_xy'] = new_masks\n",
    "resized_dataset_df['yolo_txt_path'] = txt_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a47d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atualiza os metadados do conjunto redimensionado\n",
    "resized_dataset_df.to_csv('resized_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38939717",
   "metadata": {},
   "source": [
    "# Geração dos Folds para Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87bf352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lê novamente os metadados do conjunto redimensionado em CSV\n",
    "resized_dataset_df = pd.read_csv('resized_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bf88e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...: 100%|██████████| 5/5 [00:00<00:00, 294.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# realiza a divisão do dataframe em folds de treinamento e validação\n",
    "kfolder = DataFrameKFoldSplitter(resized_dataset_df, n_splits=5, random_state=42)\n",
    "folds = kfolder.split_folds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e25449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define o caminho e criação do diretório para o conjunto com os folds\n",
    "dataset_folds_path = Path('dataset_folds/')\n",
    "os.makedirs(dataset_folds_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c03978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAML criado em: dataset_folds\\fold_1\\dataset.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...:  20%|██        | 1/5 [00:27<01:51, 27.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAML criado em: dataset_folds\\fold_2\\dataset.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...:  40%|████      | 2/5 [00:28<00:35, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAML criado em: dataset_folds\\fold_3\\dataset.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...:  60%|██████    | 3/5 [00:29<00:13,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAML criado em: dataset_folds\\fold_4\\dataset.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...:  80%|████████  | 4/5 [00:30<00:04,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAML criado em: dataset_folds\\fold_5\\dataset.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...: 100%|██████████| 5/5 [00:31<00:00,  6.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# realiza a criação dos diretórios dos folds e respectiva cópia dos dados na estrutura esperada pela Ultralytics\n",
    "# cada fold também conta com o arquivo YAML com os caminhos até os dados de treinamento e validação esperado pela Ultralytics \n",
    "# por fim, cada fold também conta com um arquvio dataset.csv com os respectivos metadados daquele fold\n",
    "\n",
    "for index, fold in enumerate(tqdm(folds, total=len(folds), desc='Gerando folds...')):\n",
    "    images_path = []\n",
    "    labels_path = []\n",
    "\n",
    "    dataset_fold_columns = ['image_path', 'label_path', 'split', 'fold', 'image_width', 'image_height', 'xy', 'yolo_xy']\n",
    "    dataset_fold_df = pd.DataFrame(columns=dataset_fold_columns)\n",
    "\n",
    "    fold_path = dataset_folds_path/f'fold_{index+1}'\n",
    "    \n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "        \n",
    "        ICDARYOLOConverter.create_folders(fold_path)\n",
    "\n",
    "        splits = ['train', 'val']\n",
    "\n",
    "        for split in splits:\n",
    "        \n",
    "            fold_images_path = fold_path/split/'images'\n",
    "            fold_labels_path = fold_path/split/'labels'\n",
    "\n",
    "            original_images_path_list = fold[split]['image_path']\n",
    "            for original_image_path in original_images_path_list:\n",
    "                shutil.copy(original_image_path , fold_images_path.as_posix())\n",
    "                \n",
    "            original_labels_path_list = fold[split]['yolo_txt_path']\n",
    "            for original_label_path in original_labels_path_list:\n",
    "                shutil.copy(original_label_path, fold_labels_path.as_posix())\n",
    "\n",
    "\n",
    "            images_path = [fold_images_path/image_basename for image_basename in os.listdir(fold_images_path)]\n",
    "            labels_path = [fold_labels_path/label_basename for label_basename in os.listdir(fold_labels_path)]\n",
    "\n",
    "            temp_df = pd.DataFrame({\n",
    "                'image_path': images_path, \n",
    "                'fold_path': labels_path,\n",
    "                'split':  fold[split].shape[0] * [split] ,\n",
    "                'fold':  fold[split].shape[0] * [index+1],\n",
    "                'image_width': fold[split]['image_width'],\n",
    "                'image_height': fold[split]['image_height'],\n",
    "                'xy': fold[split]['xy'],\n",
    "                'yolo_xy': fold[split]['yolo_xy']})\n",
    "            \n",
    "            dataset_fold_df = pd.concat([dataset_fold_df, temp_df], axis=1)\n",
    "\n",
    "\n",
    "        ICDARYOLOConverter.create_yaml(\n",
    "            output_dir=fold_path,\n",
    "            train_fold_path=fold_path/'train',\n",
    "            val_fold_path=fold_path/'val'\n",
    "        )\n",
    "\n",
    "        dataset_fold_df.to_csv(fold_path/'dataset.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7229ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove o diretório do dataset redimensionado, já que ele não será necessário\n",
    "shutil.rmtree('resized_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c823b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarta o arquivo resized_dataset.csv, já que ele não é necessário também\n",
    "os.remove('resized_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206ba86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db15fcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5625d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto_redes_neurais",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
