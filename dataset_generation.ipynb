{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fada139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1257734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataExtractor.downloader import download_dataset\n",
    "from DataExtractor.dataset_to_dataframe import ConvertICDARDatasetToDataframe\n",
    "from DataAugmentation.augmentation import Augmentation\n",
    "from DataExtractor.yolo_converter import ICDARYOLOConverter\n",
    "from DataSplitter.kfold import DataFrameKFoldSplitter\n",
    "from DataAugmentation.augmentation import Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4acf45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6561dfc",
   "metadata": {},
   "source": [
    "# Gera√ß√£o Treino/Valida√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89857ae",
   "metadata": {},
   "source": [
    "## Download do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25704979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset j√° existe!\n"
     ]
    }
   ],
   "source": [
    "# realiza o download do dataset\n",
    "\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad2b20",
   "metadata": {},
   "source": [
    "## Extra√ß√£o de Metadados em DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be723b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Associando pares... : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 2839.46it/s]\n",
      "Associando pares... : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 2839.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando DataFrame do conjnuto de dados...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo dimens√£o das imagens...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 11671.53it/s]\n",
      "Extraindo dimens√£o das imagens...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 11671.53it/s]\n",
      "Extraindo as anota√ß√µes...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:01<00:00, 304.90it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gera um dataframe contendo metadados sobre o dataset como o caminhos at√© as imagens, ss dimens√µes das imagens, \n",
    "# as anota√ß√µes dos pontos (x,y) dos v√©rtices dos pol√≠gonos das m√°scaradas extra√≠das a partir dos arquivos XML, etc.\n",
    "\n",
    "converter = ConvertICDARDatasetToDataframe(\n",
    "    images_path=\"./dataset/training/TRACKB1/ground_truth\",\n",
    "    labels_path=\"./dataset/training/TRACKB1/ground_truth\"\n",
    ")\n",
    "\n",
    "df = converter.generate_dataframe()\n",
    "df.to_csv('dataset.csv',  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2335e6",
   "metadata": {},
   "source": [
    "## Redimensionamento das Imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9570a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realiza a leitura dos metadados do conjunto de dados no arquivo dataset.csv\n",
    "dataset_df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# cria uma nova pasta para armazenar as imagens redimensionadas \n",
    "os.makedirs('resized_dataset', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb481bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redimensionando as imagens e anota√ß√£oes para 640X640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convertendo imagem cTDaR_t00930.jpg (2480X3830) -> (640X640): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [02:35<00:00,  3.59it/s] \n",
      "Convertendo imagem cTDaR_t00930.jpg (2480X3830) -> (640X640): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [02:35<00:00,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Redimensiona as imagens para o tamanho 640x640, al√©m de redimensionar as anota√ß√µes respectivamente\n",
    "\n",
    "new_width, new_height = 640, 640\n",
    "total_rows = dataset_df.shape[0]\n",
    "\n",
    "new_masks = []\n",
    "new_paths = []\n",
    "\n",
    "print(f'Redimensionando as imagens e anota√ß√£oes para {new_width}X{new_height}')\n",
    "\n",
    "pbar = tqdm(dataset_df.iterrows(),total=dataset_df.shape[0])\n",
    "\n",
    "for _, dataset_row in pbar: \n",
    "\n",
    "    \n",
    "    image_path  = dataset_row['image_path']\n",
    "    masks = json.loads(dataset_row['xy'])\n",
    "\n",
    "    image_filename = os.path.basename(image_path)\n",
    "    output_path = f'./resized_dataset/{image_filename}'\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = np.asarray(image)\n",
    "\n",
    "    pbar.set_description(f'Convertendo imagem {image_filename} ({image.shape[1]}X{image.shape[0]}) -> ({new_width}X{new_height})')\n",
    "    \n",
    "    resized_image, resized_masks = \\\n",
    "        Augmentation.resize_image(image, new_width, new_height, masks=masks)\n",
    "    \n",
    "    new_masks.append(resized_masks)\n",
    "    new_paths.append(output_path)\n",
    "\n",
    "    Augmentation.save_image(resized_image, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a30a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove o diret√≥rio do dataset original, j√° que ele n√£o ser√° necess√°rio\n",
    "shutil.rmtree('dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b3caaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarta o arquivo dataset.csv, j√° que ele n√£o √© necess√°rio tamb√©m\n",
    "os.remove('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a72b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# armazena os metadados do conjunto redimensionado em um arquivo csv\n",
    "resized_dataset_df = pd.DataFrame(\n",
    "    {'image_path':new_paths,\n",
    "    'xy':new_masks}\n",
    ")\n",
    "\n",
    "resized_dataset_df['image_width'] = new_width\n",
    "resized_dataset_df['image_height'] = new_height\n",
    "\n",
    "resized_dataset_df.to_csv('resized_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57c805",
   "metadata": {},
   "source": [
    "## Gera√ß√£o das Anota√ß√µes YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb9254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leitura do metadados do conjunto de dados redimensionado\n",
    "resized_dataset_df = pd.read_csv('resized_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3bd9ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando anota√ß√µes YOLO em txt...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:01<00:00, 341.15it/s]\n",
      "Gerando anota√ß√µes YOLO em txt...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:01<00:00, 341.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# gera√ß√£o dos r√≥tulos YOLO em arquivos txt no mesmo diret√≥rio das respectivas imagens\n",
    "\n",
    "new_masks = []\n",
    "txt_paths = []\n",
    "\n",
    "for _, resized_dataset_row in tqdm(resized_dataset_df.iterrows(),  \n",
    "                                   total = resized_dataset_df.shape[0],\n",
    "                                   desc='Gerando anota√ß√µes YOLO em txt...') :\n",
    "    \n",
    "    image_width = resized_dataset_row['image_width']\n",
    "    image_height = resized_dataset_row['image_height']\n",
    "    image_path = resized_dataset_row['image_path']\n",
    "    masks = resized_dataset_row['xy']\n",
    "    masks = json.loads(masks)\n",
    "\n",
    "    image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    image_path = Path(os.path.dirname(image_path))\n",
    "    txt_path = image_path/f'{image_filename}.txt' \n",
    "\n",
    "    \n",
    "    yolo_normalized_masks = ICDARYOLOConverter.process_masks(masks, image_width, image_height, txt_path)\n",
    "\n",
    "    new_masks.append(yolo_normalized_masks)\n",
    "    txt_paths.append(txt_path.as_posix())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f38ae201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adiciona as m√°scarar convertidas para o formato YOLO e os respectivos caminhos para os arquivos TXT com as anota√ß√µes salva\n",
    "resized_dataset_df['yolo_xy'] = new_masks\n",
    "resized_dataset_df['yolo_txt_path'] = txt_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a47d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atualiza os metadados do conjunto redimensionado\n",
    "resized_dataset_df.to_csv('resized_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38939717",
   "metadata": {},
   "source": [
    "## Gera√ß√£o dos Folds para Valida√ß√£o Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87bf352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l√™ novamente os metadados do conjunto redimensionado em CSV\n",
    "resized_dataset_df = pd.read_csv('resized_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bf88e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 169.05it/s]\n",
      "Gerando folds...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 169.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# realiza a divis√£o do dataframe em folds de treinamento e valida√ß√£o\n",
    "kfolder = DataFrameKFoldSplitter(resized_dataset_df, n_splits=5, random_state=42)\n",
    "folds = kfolder.split_folds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e25449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define o caminho e cria√ß√£o do diret√≥rio para o conjunto com os folds\n",
    "dataset_folds_path = Path('dataset_folds/')\n",
    "os.makedirs(dataset_folds_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c03978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAML criado em: dataset_folds/fold_1/dataset.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...:  20%|‚ñà‚ñà        | 1/5 [00:00<00:01,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAML criado em: dataset_folds/fold_2/dataset.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:00<00:01,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAML criado em: dataset_folds/fold_3/dataset.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:01<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAML criado em: dataset_folds/fold_4/dataset.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:01<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAML criado em: dataset_folds/fold_5/dataset.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando folds...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.24it/s]\n",
      "Gerando folds...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# realiza a cria√ß√£o dos diret√≥rios dos folds e respectiva c√≥pia dos dados na estrutura esperada pela Ultralytics\n",
    "# cada fold tamb√©m conta com o arquivo YAML com os caminhos at√© os dados de treinamento e valida√ß√£o esperado pela Ultralytics \n",
    "# por fim, cada fold tamb√©m conta com um arquvio dataset.csv com os respectivos metadados daquele fold\n",
    "\n",
    "for index, fold in enumerate(tqdm(folds, total=len(folds), desc='Gerando folds...')):\n",
    "    images_path = []\n",
    "    labels_path = []\n",
    "\n",
    "    dataset_fold_columns = ['image_path', 'label_path', 'split', 'fold', 'image_width', 'image_height', 'xy', 'yolo_xy']\n",
    "    dataset_fold_df = pd.DataFrame(columns=dataset_fold_columns)\n",
    "\n",
    "    fold_path = dataset_folds_path/f'fold_{index+1}'\n",
    "    \n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "        \n",
    "        ICDARYOLOConverter.create_folders(fold_path)\n",
    "\n",
    "        splits = ['train', 'val']\n",
    "\n",
    "        for split in splits:\n",
    "        \n",
    "            fold_images_path = fold_path/split/'images'\n",
    "            fold_labels_path = fold_path/split/'labels'\n",
    "\n",
    "            original_images_path_list = fold[split]['image_path']\n",
    "            for original_image_path in original_images_path_list:\n",
    "                shutil.copy(original_image_path , fold_images_path.as_posix())\n",
    "                \n",
    "            original_labels_path_list = fold[split]['yolo_txt_path']\n",
    "            for original_label_path in original_labels_path_list:\n",
    "                shutil.copy(original_label_path, fold_labels_path.as_posix())\n",
    "\n",
    "\n",
    "            images_path = [fold_images_path/image_basename for image_basename in os.listdir(fold_images_path)]\n",
    "            labels_path = [fold_labels_path/label_basename for label_basename in os.listdir(fold_labels_path)]\n",
    "\n",
    "            temp_df = pd.DataFrame({\n",
    "                'image_path': images_path, \n",
    "                'fold_path': labels_path,\n",
    "                'split':  fold[split].shape[0] * [split] ,\n",
    "                'fold':  fold[split].shape[0] * [index+1],\n",
    "                'image_width': fold[split]['image_width'],\n",
    "                'image_height': fold[split]['image_height'],\n",
    "                'xy': fold[split]['xy'],\n",
    "                'yolo_xy': fold[split]['yolo_xy']})\n",
    "            \n",
    "            dataset_fold_df = pd.concat([dataset_fold_df, temp_df], axis=1)\n",
    "\n",
    "\n",
    "        ICDARYOLOConverter.create_yaml(\n",
    "            output_dir=fold_path,\n",
    "            train_fold_path=fold_path/'train',\n",
    "            val_fold_path=fold_path/'val'\n",
    "        )\n",
    "\n",
    "        dataset_fold_df.to_csv(fold_path/'dataset.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7229ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove o diret√≥rio do dataset redimensionado, j√° que ele n√£o ser√° necess√°rio\n",
    "shutil.rmtree('resized_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c823b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarta o arquivo resized_dataset.csv, j√° que ele n√£o √© necess√°rio tamb√©m\n",
    "os.remove('resized_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac14be5c",
   "metadata": {},
   "source": [
    "## Aplica√ß√£o das Transforma√ß√µes de Aumento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6525c5a",
   "metadata": {},
   "source": [
    "**ATEN√á√ÉO**: N√ÉO √â POSS√çVEL REPRODUZIR AS TRANSFORMA√á√ïES APLICADAS AQUI. PORTANTO, UMA VEZ GERADO, O MESMO DATASET DEVE SER EMPREGADO NO DRIVE. A ALEATORIEDADE DAS TRANSFORMA√á√ïES INTRODUZ DIVERSIDADE NO AUMENTO DE DADOS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eba427ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aplicando transforma√ß√µes...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 10.29it/s]\n",
      "Aplicando transforma√ß√µes...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 10.29it/s]\n"
     ]
    }
   ],
   "source": [
    "transforms = [A.GaussianBlur(p=1.0), A.SaltAndPepper(p=1.0), A.RandomBrightnessContrast(p=1.0)]\n",
    "\n",
    "for fold_index in tqdm(range(1, 6), desc='Aplicando transforma√ß√µes...'):\n",
    "\n",
    "    images_folder_path = Path(f'dataset_folds/fold_{fold_index}/train/images/')\n",
    "    labels_folder_path = Path(f'dataset_folds/fold_{fold_index}/train/labels/')\n",
    "\n",
    "    images_path = [images_folder_path/path for path in os.listdir(images_folder_path)]\n",
    "\n",
    "    for image_path in images_path[0:5]:\n",
    "        for index, transform in enumerate(transforms):\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            image_filename, image_format = os.path.splitext(os.path.basename(image_path))\n",
    "            \n",
    "            new_image = Augmentation.apply_albumentation_tranform(image, transform)['image']\n",
    "\n",
    "            new_image_path = images_folder_path/f'{image_filename}_var_{index+1}.{image_format}'\n",
    "\n",
    "            Augmentation.save_image(new_image, new_image_path)\n",
    "            shutil.copy(labels_folder_path/f'{image_filename}.txt',  labels_folder_path/f'{image_filename}_var_{index+1}.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6068e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d54feddb",
   "metadata": {},
   "source": [
    "# Treinamento do YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2765dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1112af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando modelo: yolov8n.pt\n",
      "\n",
      "Treinando FOLD 1 com yolov8n.pt \n",
      "\n",
      "YAML: dataset_folds/fold_1/dataset.yaml\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.2MB 7.9MB/s 0.8s0.7s<0.1s.1s\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.2MB 7.9MB/s 0.8s\n",
      "Ultralytics 8.3.233 üöÄ Python-3.13.5 torch-2.9.1+cu128 CPU (AMD Ryzen 5 7535U with Radeon Graphics)\n",
      "Ultralytics 8.3.233 üöÄ Python-3.13.5 torch-2.9.1+cu128 CPU (AMD Ryzen 5 7535U with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_folds/fold_1/dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_fold_1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs_yolo_folds_all, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/williancarddd/Projects/workspace-usp/ocr-table-recognition/runs_yolo_folds_all/yolov8n_fold_1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_folds/fold_1/dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_fold_1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs_yolo_folds_all, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/williancarddd/Projects/workspace-usp/ocr-table-recognition/runs_yolo_folds_all/yolov8n_fold_1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/home/williancarddd/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 4.0MB/s 0.2s 0.1s<0.1s\n",
      "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/home/williancarddd/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 4.0MB/s 0.2s\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2040.9¬±1118.8 MB/s, size: 285.4 KB)\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2040.9¬±1118.8 MB/s, size: 285.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/williancarddd/Projects/workspace-usp/ocr-table-recognition/dataset_folds/fold_1/train/labels... 446 images, 15 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 461/461 333.6it/s 1.4s0.1s\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/williancarddd/Projects/workspace-usp/ocr-table-recognition/dataset_folds/fold_1/train/labels... 446 images, 15 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 461/461 333.6it/s 1.4s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/williancarddd/Projects/workspace-usp/ocr-table-recognition/dataset_folds/fold_1/train/labels.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/williancarddd/Projects/workspace-usp/ocr-table-recognition/dataset_folds/fold_1/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1623.4¬±341.4 MB/s, size: 50.6 KB)\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1623.4¬±341.4 MB/s, size: 50.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/williancarddd/Projects/workspace-usp/ocr-table-recognition/dataset_folds/fold_1/val/labels... 112 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 112/112 266.5it/s 0.4s1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/williancarddd/Projects/workspace-usp/ocr-table-recognition/dataset_folds/fold_1/val/images/cTDaR_t00126.jpg: 10 duplicate labels removed\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/williancarddd/Projects/workspace-usp/ocr-table-recognition/dataset_folds/fold_1/val/labels... 112 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 112/112 266.5it/s 0.4s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/williancarddd/Projects/workspace-usp/ocr-table-recognition/dataset_folds/fold_1/val/images/cTDaR_t00126.jpg: 10 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/williancarddd/Projects/workspace-usp/ocr-table-recognition/dataset_folds/fold_1/val/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/williancarddd/Projects/workspace-usp/ocr-table-recognition/dataset_folds/fold_1/val/labels.cache\n",
      "Plotting labels to /home/williancarddd/Projects/workspace-usp/ocr-table-recognition/runs_yolo_folds_all/yolov8n_fold_1/labels.jpg... \n",
      "Plotting labels to /home/williancarddd/Projects/workspace-usp/ocr-table-recognition/runs_yolo_folds_all/yolov8n_fold_1/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/home/williancarddd/Projects/workspace-usp/ocr-table-recognition/runs_yolo_folds_all/yolov8n_fold_1\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/home/williancarddd/Projects/workspace-usp/ocr-table-recognition/runs_yolo_folds_all/yolov8n_fold_1\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50         0G      3.187      3.521      2.215       3515        640: 14% ‚îÅ‚ï∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 4/29 12.8s/it 57.1s<5:20\n",
      "\u001b[K       1/50         0G      3.187      3.521      2.215       3515        640: 14% ‚îÅ‚ï∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 4/29 12.8s/it 57.1s<5:20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Nome do experimento (separa por modelo e fold)\u001b[39;00m\n\u001b[32m     32\u001b[39m run_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(model_name).stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m results_train = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_yaml_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTreinamento do fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m com \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m conclu√≠do.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/workspace-usp/ocr-table-recognition/.venv/lib/python3.13/site-packages/ultralytics/engine/model.py:773\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/workspace-usp/ocr-table-recognition/.venv/lib/python3.13/site-packages/ultralytics/engine/trainer.py:243\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    240\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/workspace-usp/ocr-table-recognition/.venv/lib/python3.13/site-packages/ultralytics/engine/trainer.py:434\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28mself\u001b[39m.tloss = \u001b[38;5;28mself\u001b[39m.loss_items \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.tloss * i + \u001b[38;5;28mself\u001b[39m.loss_items) / (i + \u001b[32m1\u001b[39m)\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/workspace-usp/ocr-table-recognition/.venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/workspace-usp/ocr-table-recognition/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/workspace-usp/ocr-table-recognition/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "base_folds_path = Path(\"dataset_folds\")\n",
    "project_name = \"runs_yolo_folds_all\"\n",
    "\n",
    "# Lista de modelos YOLO\n",
    "model_names = [\n",
    "    \"yolov8n.pt\",\n",
    "    \"yolov9n.pt\",\n",
    "    \"yolov10n.pt\",\n",
    "    \"yolo11n.pt\",\n",
    "]\n",
    "\n",
    "# Hiperpar√¢metros\n",
    "epochs = 50\n",
    "imgsz = 640\n",
    "batch = 16\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Treinando modelo: {model_name}\\n\")\n",
    "\n",
    "    for fold_idx in range(1, 6):\n",
    "        print(f\"Treinando FOLD {fold_idx} com {model_name} \\n\")\n",
    "\n",
    "        fold_path = base_folds_path / f\"fold_{fold_idx}\"\n",
    "        data_yaml_path = fold_path / \"dataset.yaml\"  \n",
    "\n",
    "        print(\"YAML:\", data_yaml_path)\n",
    "\n",
    "        # Carrega o modelo correspondente\n",
    "        model = YOLO(model_name)\n",
    "\n",
    "        # Nome do experimento (separa por modelo e fold)\n",
    "        run_name = f\"{Path(model_name).stem}_fold_{fold_idx}\"\n",
    "\n",
    "        results_train = model.train(\n",
    "            data=str(data_yaml_path),\n",
    "            epochs=epochs,\n",
    "            imgsz=imgsz,\n",
    "            batch=batch,\n",
    "            project=project_name,\n",
    "            name=run_name,\n",
    "            exist_ok=True,\n",
    "        )\n",
    "\n",
    "        print(f\"Treinamento do fold {fold_idx} com {model_name} conclu√≠do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c8239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed4cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21735b7c",
   "metadata": {},
   "source": [
    "# Gera√ß√£o Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363ea15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe580b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110cab20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0ac96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586348b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc1147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ffc20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206ba86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db15fcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5625d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr-table-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
